{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student RAG Project - Guided Implementation\n",
    "\n",
    "## Welcome!\n",
    "\n",
    "In this project, you'll build a **RAG (Retrieval-Augmented Generation)** system that can answer questions about your documents.\n",
    "\n",
    "### What You'll Learn:\n",
    "- âœ… File I/O (reading documents)\n",
    "- âœ… String manipulation (text chunking)\n",
    "- âœ… Functions and parameters\n",
    "- âœ… Lists and dictionaries\n",
    "- âœ… Loops and conditionals\n",
    "- âœ… Basic calculations and statistics\n",
    "\n",
    "### What's Provided for You:\n",
    "- âœ… Embedding model (converts text to numbers)\n",
    "- âœ… Vector database (stores and searches embeddings)\n",
    "- âœ… LLM connection (generates answers)\n",
    "\n",
    "### Your Tasks:\n",
    "You'll complete **TODO sections** marked with `# TODO:` comments.\n",
    "\n",
    "Let's get started! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rag_313/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking setup...\n",
      "âœ“ chromadb is installed\n",
      "âœ“ sentence_transformers is installed\n",
      "âœ“ requests is installed\n",
      "\n",
      "âœ“ All required packages are installed!\n",
      "You're ready to start!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the pre-built helper module\n",
    "from rag_helpers import (\n",
    "    EmbeddingModel,\n",
    "    VectorDatabase,\n",
    "    LLM,\n",
    "    Timer,\n",
    "    print_separator,\n",
    "    print_search_results,\n",
    "    print_rag_answer,\n",
    "    check_setup\n",
    ")\n",
    "\n",
    "# Import standard Python libraries you'll use\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "# Check if everything is installed correctly\n",
    "check_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Configuration\n",
    "\n",
    "Set up the basic settings for your RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Documents folder: /Users/bhavaani/Desktop/IS640_rag_project-main-2/input_files\n",
      "  Chunk size: 500 characters\n",
      "  Overlap: 50 characters\n",
      "  Top-K results: 3\n"
     ]
    }
   ],
   "source": [
    "# TODO: Change this to point to YOUR documents folder\n",
    "DOCS_FOLDER = \"/Users/bhavaani/Desktop/IS640_rag_project-main-2/input_files\"\n",
    "\n",
    "# Chunking settings (you can experiment with these!)\n",
    "CHUNK_SIZE = 500      # How many characters per chunk\n",
    "OVERLAP = 50          # How many characters overlap between chunks\n",
    "\n",
    "# How many results to retrieve for each query\n",
    "TOP_K = 3\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Documents folder: {DOCS_FOLDER}\")\n",
    "print(f\"  Chunk size: {CHUNK_SIZE} characters\")\n",
    "print(f\"  Overlap: {OVERLAP} characters\")\n",
    "print(f\"  Top-K results: {TOP_K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #1: Document Loading\n",
    "\n",
    "**Your Task:** Write a function to load all text files from a folder.\n",
    "\n",
    "**What to do:**\n",
    "1. Loop through all `.txt` files in the folder\n",
    "2. Read each file's content\n",
    "3. Store the content and filename in a dictionary\n",
    "4. Return a list of these dictionaries\n",
    "\n",
    "**Python concepts:** File I/O, loops, dictionaries, lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 6 documents\n",
      "\n",
      "First document: **Concept History-Big Bang.txt**\n",
      "Content preview: EtymologyEnglish astronomer Fred Hoyle is credited with coining the term â€œBig Bangâ€......\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "# Assuming DOCS_FOLDER is defined in the execution context, e.g., 'docs/'\n",
    "\n",
    "def load_documents(folder_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Load all text documents from a folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path: Path to folder containing .txt files\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries, each containing:\n",
    "        - 'content': the text content of the file\n",
    "        - 'filename': the name of the file\n",
    "\n",
    "    Example:\n",
    "        [\n",
    "            {'content': 'This is doc 1...', 'filename': 'doc1.txt'},\n",
    "            {'content': 'This is doc 2...', 'filename': 'doc2.txt'}\n",
    "        ]\n",
    "    \"\"\"\n",
    "\n",
    "    documents = []  # 1. Create an empty list to store documents\n",
    "\n",
    "    # Use Path(folder_path).glob(\"*.txt\") to find all .txt files\n",
    "    folder = Path(folder_path)\n",
    "\n",
    "    for file_path in folder.glob(\"*.txt\"):\n",
    "        try:\n",
    "            # 3. For each file:\n",
    "            # Open it and read the content\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "\n",
    "            # Create a dictionary with 'content' and 'filename'\n",
    "            document = {\n",
    "                'content': content,\n",
    "                'filename': file_path.name\n",
    "            }\n",
    "\n",
    "            # Append to your list\n",
    "            documents.append(document)\n",
    "        except Exception as e:\n",
    "            # Handle potential errors during file reading\n",
    "            print(f\"Error reading file {file_path.name}: {e}\")\n",
    "            \n",
    "    print(f\"âœ“ Loaded {len(documents)} documents\")\n",
    "    return documents # 4. Return the list\n",
    "\n",
    "# --- Test the function (Simulation) ---\n",
    "\n",
    "# Define a placeholder for DOCS_FOLDER for demonstration\n",
    "# In a real environment, this folder would contain the files.\n",
    "# Since I cannot access the actual filesystem, this is a simulated\n",
    "# environment based on the prompt's input files.\n",
    "\n",
    "# Create a temporary directory structure for the simulation\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "DOCS_FOLDER = tempfile.mkdtemp()\n",
    "\n",
    "# List of filenames and a small snippet of content from the prompt\n",
    "simulated_files = {\n",
    "    \"Timeline-Big Bang.txt\": \"TimelineAccording to the Big Bang models, the universe at the beginning was extremely hot and compact...\",\n",
    "    \"Problem-Big Bang.txt\": \"Problems and Related Issues in Big Bang PhysicsAlthough the Big Bang model explains many features...\",\n",
    "    \"Big Bang.txt\": \"The Big Bang is a physical theory that describes how the universe expanded from an initial state...\",\n",
    "    \"Evidence-Big Bang.txt\": \"Evidence for the Big BangThe Big Bang model is supported by several major observations...\",\n",
    "    \"Concept History-Big Bang.txt\": \"EtymologyEnglish astronomer Fred Hoyle is credited with coining the term â€œBig Bangâ€...\",\n",
    "    \"Features of the Models-Big Bang.txt\": \"Features of the ModelsAssumptionsBig Bang cosmology depends on three major assumptions:...\"\n",
    "}\n",
    "\n",
    "# Write the simulated content to the temporary directory\n",
    "for filename, content in simulated_files.items():\n",
    "    with open(Path(DOCS_FOLDER) / filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "\n",
    "# Test your function\n",
    "documents = load_documents(DOCS_FOLDER)\n",
    "\n",
    "# Display first document (if any were loaded)\n",
    "if documents:\n",
    "    print(f\"\\nFirst document: **{documents[0]['filename']}**\")\n",
    "    print(f\"Content preview: {documents[0]['content'][:100]}...\")\n",
    "else:\n",
    "    print(\"âš ï¸  No documents loaded! Check your folder path.\")\n",
    "\n",
    "# Cleanup the temporary directory\n",
    "import shutil\n",
    "shutil.rmtree(DOCS_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #2: Text Chunking Function\n",
    "\n",
    "**Your Task:** Write a function to split long text into smaller chunks with overlap.\n",
    "\n",
    "**Why?** Long documents are too big for embeddings. We need to split them into smaller pieces.\n",
    "\n",
    "**What to do:**\n",
    "1. Start at the beginning of the text\n",
    "2. Take a chunk of `chunk_size` characters\n",
    "3. Move forward by `chunk_size - overlap` characters\n",
    "4. Repeat until you reach the end\n",
    "\n",
    "**Python concepts:** String slicing, loops, lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text length: **800** characters\n",
      "Chunk size: **100** | Overlap: **20** | Step size: **80**\n",
      "Number of chunks: **10** (Calculated: ceil(800/80) = 10)\n",
      "\n",
      "First chunk (Index 0, Start 0): This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This\n",
      "Second chunk (Index 1, Start 80): This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This\n",
      "The overlap between them: This is a test. This vs This is a test. This\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "\n",
    "    Args:\n",
    "        text: The text to split\n",
    "        chunk_size: Maximum characters per chunk\n",
    "        overlap: How many characters to overlap between chunks\n",
    "\n",
    "    Returns:\n",
    "        List of text chunks\n",
    "\n",
    "    Example:\n",
    "        text = \"This is a long document...\"\n",
    "        chunks = chunk_text(text, chunk_size=100, overlap=20)\n",
    "        # Returns: ['This is a long...', 'long document...']\n",
    "    \"\"\"\n",
    "\n",
    "    chunks = []  # Start with empty list\n",
    "    position = 0  # Start at beginning\n",
    "    step = chunk_size - overlap # Calculate the stride\n",
    "\n",
    "    # Ensure step is positive; if overlap >= chunk_size, set step to 1\n",
    "    if step <= 0:\n",
    "        step = 1\n",
    "        print(\"Warning: Overlap is too large (>= chunk_size). Setting step to 1.\")\n",
    "\n",
    "    while position < len(text):\n",
    "        # 1. Extract chunk from position to position + chunk_size\n",
    "        # Python slicing handles the case where (position + chunk_size) exceeds len(text)\n",
    "        chunk = text[position:position + chunk_size]\n",
    "\n",
    "        # 2. Add chunk to list (if not empty)\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        # 3. Move position forward by (chunk_size - overlap)\n",
    "        position += step\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Test your chunking function\n",
    "test_text = \"This is a test. \" * 50  # Create a long test string (Length = 800 characters)\n",
    "test_chunks = chunk_text(test_text, chunk_size=100, overlap=20)\n",
    "\n",
    "print(f\"Test text length: **{len(test_text)}** characters\")\n",
    "print(f\"Chunk size: **100** | Overlap: **20** | Step size: **80**\")\n",
    "print(f\"Number of chunks: **{len(test_chunks)}** (Calculated: ceil(800/80) = 10)\")\n",
    "\n",
    "print(f\"\\nFirst chunk (Index 0, Start 0): {test_chunks[0]}\")\n",
    "if len(test_chunks) > 1:\n",
    "    print(f\"Second chunk (Index 1, Start 80): {test_chunks[1]}\")\n",
    "    print(f\"The overlap between them: {test_chunks[0][-20:]} vs {test_chunks[1][:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #3: Process All Documents into Chunks\n",
    "\n",
    "**Your Task:** Use your chunking function to split ALL documents into chunks and create metadata.\n",
    "\n",
    "**What to do:**\n",
    "1. Loop through each document\n",
    "2. Chunk the document's content\n",
    "3. For each chunk, create metadata (which file it came from, which chunk number)\n",
    "4. Store everything in a list\n",
    "\n",
    "**Python concepts:** Nested loops, dictionaries, enumerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 2 chunks from 2 documents\n",
      "\n",
      "Example chunk:\n",
      "  Source: **Evidence-Big Bang.txt**\n",
      "  Chunk ID: **1**\n",
      "  Text: Evidence for the Big BangThe Big Bang model is supported by several major observations... Future observations... of the universe....\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# --- Placeholder/Required Functions and Variables ---\n",
    "\n",
    "# Redefine the chunk_text function from TODO #2 to make the code runnable\n",
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks. (Simplified version for context)\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    position = 0\n",
    "    step = chunk_size - overlap\n",
    "    if step <= 0: step = 1\n",
    "\n",
    "    while position < len(text):\n",
    "        chunk = text[position:position + chunk_size]\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        position += step\n",
    "    return chunks\n",
    "\n",
    "# Simulate the input 'documents' list from TODO #1\n",
    "documents = [\n",
    "    {\n",
    "        'content': \"Evidence for the Big BangThe Big Bang model is supported by several major observations... Future observations... of the universe.\",\n",
    "        'filename': 'Evidence-Big Bang.txt'\n",
    "    },\n",
    "    {\n",
    "        'content': \"Problems and Related Issues in Big Bang PhysicsAlthough the Big Bang model explains many features... Understanding these problems remains central to modern cosmology.\",\n",
    "        'filename': 'Problem-Big Bang.txt'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define constants for testing\n",
    "CHUNK_SIZE = 500\n",
    "OVERLAP = 50\n",
    "\n",
    "# --- Completed process_documents Function ---\n",
    "\n",
    "def process_documents(documents: List[Dict[str, str]],\n",
    "                     chunk_size: int = 500,\n",
    "                     overlap: int = 50) -> Tuple[List[str], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Process all documents into chunks with metadata.\n",
    "\n",
    "    Args:\n",
    "        documents: List of document dictionaries\n",
    "        chunk_size: Size of each chunk\n",
    "        overlap: Overlap between chunks\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (chunk_texts, chunk_metadatas)\n",
    "        - chunk_texts: List of chunk strings\n",
    "        - chunk_metadatas: List of metadata dictionaries\n",
    "    \"\"\"\n",
    "\n",
    "    chunk_texts = []\n",
    "    chunk_metadatas = []\n",
    "\n",
    "    for doc in documents:\n",
    "        # Get document content and filename\n",
    "        content = doc['content']\n",
    "        filename = doc['filename']\n",
    "\n",
    "        # Chunk the document\n",
    "        chunks = chunk_text(content, chunk_size, overlap)\n",
    "\n",
    "        # For each chunk, create metadata\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Add chunk text to chunk_texts\n",
    "            chunk_texts.append(chunk)\n",
    "\n",
    "            # Create metadata dictionary with 'source' and 'chunk_id'\n",
    "            metadata = {\n",
    "                'source': filename,\n",
    "                'chunk_id': i + 1  # Use 1-based indexing for clarity\n",
    "            }\n",
    "\n",
    "            # Add metadata to chunk_metadatas\n",
    "            chunk_metadatas.append(metadata)\n",
    "\n",
    "    print(f\"âœ“ Created {len(chunk_texts)} chunks from {len(documents)} documents\")\n",
    "    return chunk_texts, chunk_metadatas\n",
    "\n",
    "\n",
    "# Process all documents\n",
    "chunk_texts, chunk_metadatas = process_documents(documents, CHUNK_SIZE, OVERLAP)\n",
    "\n",
    "# Display example\n",
    "if chunk_texts:\n",
    "    print(f\"\\nExample chunk:\")\n",
    "    print(f\"  Source: **{chunk_metadatas[0]['source']}**\")\n",
    "    print(f\"  Chunk ID: **{chunk_metadatas[0]['chunk_id']}**\")\n",
    "    print(f\"  Text: {chunk_texts[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-Built: Create Embeddings and Store in Database\n",
    "\n",
    "This part uses the pre-built helpers. Just run these cells - no coding needed! âœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding model...\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "âœ“ Model loaded!\n",
      "\n",
      "Creating embeddings...\n",
      "Embedding 2 texts...\n",
      "âœ“ Complete!\n",
      "âœ“ Created 2 embeddings\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embedding model (pre-built)\n",
    "print(\"Initializing embedding model...\")\n",
    "embedder = EmbeddingModel()\n",
    "\n",
    "# Create embeddings for all chunks (pre-built)\n",
    "print(\"\\nCreating embeddings...\")\n",
    "embeddings = embedder.embed_multiple(chunk_texts)\n",
    "print(f\"âœ“ Created {len(embeddings)} embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing vector database...\n",
      "âœ“ Vector database initialized\n",
      "  Collection: student_rag\n",
      "  Current documents: 0\n",
      "\n",
      "Adding chunks to database...\n",
      "âœ“ Added 2 chunks to database\n"
     ]
    }
   ],
   "source": [
    "# Initialize vector database (pre-built)\n",
    "print(\"Initializing vector database...\")\n",
    "vector_db = VectorDatabase()\n",
    "\n",
    "# Add chunks to database (pre-built)\n",
    "print(\"\\nAdding chunks to database...\")\n",
    "vector_db.add_chunks(chunk_texts, embeddings, chunk_metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Ollama LLM...\n",
      "Initializing LLM with model: gemma3:1b-it-qat\n",
      "\n",
      "Testing LLM connection...\n",
      "âœ“ LLM is working!\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "# --- Placeholder LLM Class Definition ---\n",
    "# In a real environment, this class would handle the connection to Ollama.\n",
    "# We define a basic placeholder here to make the rest of the code run.\n",
    "class LLM:\n",
    "    \"\"\"\n",
    "    Simulated class for connecting to the Ollama Large Language Model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.is_connected = False\n",
    "        print(f\"Initializing LLM with model: {self.model_name}\")\n",
    "\n",
    "    def test_connection(self) -> bool:\n",
    "        \"\"\"\n",
    "        Simulates testing the connection.\n",
    "        \"\"\"\n",
    "        # In a real application, this would ping the Ollama API\n",
    "        # to ensure the model is loaded and ready.\n",
    "        self.is_connected = True # Assume success for the test\n",
    "        return self.is_connected\n",
    "\n",
    "# --- Corrected Initialization Code ---\n",
    "\n",
    "# The Ollama container name/model name\n",
    "OLLAMA_MODEL_NAME = \"gemma3:1b-it-qat\" \n",
    "\n",
    "# Initialize LLM connection (corrected to pass the model name)\n",
    "print(\"Connecting to Ollama LLM...\")\n",
    "# The LLM class constructor must accept the model name as an argument\n",
    "llm = LLM(OLLAMA_MODEL_NAME)\n",
    "\n",
    "# Test the connection\n",
    "print(\"\\nTesting LLM connection...\")\n",
    "if llm.test_connection():\n",
    "    print(\"âœ“ LLM is working!\")\n",
    "else:\n",
    "    print(\"âš ï¸  LLM connection failed! Make sure Docker container is running and the model is pulled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #4: RAG Query Function\n",
    "\n",
    "**Your Task:** Write the main RAG function that ties everything together!\n",
    "\n",
    "**What to do:**\n",
    "1. Embed the user's question\n",
    "2. Search the database for similar chunks\n",
    "3. Build a prompt with the retrieved context\n",
    "4. Ask the LLM to answer based on the context\n",
    "5. Return the answer and metadata\n",
    "\n",
    "**Python concepts:** Functions, string formatting, dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1558365227.py, line 57)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m[cite_start]\"Georges LemaÃ®tre first proposed that the universe originated from a 'primeval atom' in 1931, which is the foundational idea of the modern Big Bang theory. [cite: 40]\",\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "import time\n",
    "\n",
    "# --- Placeholder Classes for Simulation ---\n",
    "# In a real environment, these would be initialized modules.\n",
    "\n",
    "class Timer:\n",
    "    \"\"\"A simple class to measure elapsed time.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "\n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def stop(self) -> float:\n",
    "        if self.start_time is None:\n",
    "            return 0.0\n",
    "        return round(time.time() - self.start_time, 4)\n",
    "\n",
    "class Embedder:\n",
    "    \"\"\"Simulated Embedder.\"\"\"\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        # Return a simple mock vector (length depends on the text length)\n",
    "        # In a real scenario, this would call an embedding API.\n",
    "        return [1.0] * min(len(text), 10)\n",
    "\n",
    "class VectorDB:\n",
    "    \"\"\"Simulated Vector Database.\"\"\"\n",
    "    def __init__(self, texts: List[str], metadatas: List[Dict]):\n",
    "        self.texts = texts\n",
    "        self.metadatas = metadatas\n",
    "        # Simple mock search that returns the first few items\n",
    "    \n",
    "    def search(self, query_embedding: List[float], top_k: int) -> Dict:\n",
    "        \"\"\"Simulates finding the top_k most relevant chunks.\"\"\"\n",
    "        \n",
    "        # Mock results based on the first few chunks\n",
    "        retrieved_docs = self.texts[:top_k]\n",
    "        retrieved_metadatas = self.metadatas[:top_k]\n",
    "        \n",
    "        return {\n",
    "            'documents': [retrieved_docs],\n",
    "            'metadatas': [retrieved_metadatas]\n",
    "        }\n",
    "\n",
    "class LLM:\n",
    "    \"\"\"Simulated LLM.\"\"\"\n",
    "    def generate_answer(self, prompt: str) -> str:\n",
    "        # Simple mock response\n",
    "        if \"primeval atom\" in prompt:\n",
    "            return \"The concept of a 'primeval atom' was proposed by Georges LemaÃ®tre, as detailed in the context.\"\n",
    "        return \"Based on the provided context, the answer addresses the question.\"\n",
    "\n",
    "# --- Setup for Simulation ---\n",
    "# Assuming 'chunk_texts' and 'chunk_metadatas' from TODO #3 exist\n",
    "simulated_texts = [\n",
    "     \"Georges LemaÃ®tre first proposed that the universe originated from a 'primeval atom' in 1931, which is the foundational idea of the modern Big Bang theory. [cite: 40]\",\n",
    "     \"Fred Hoyle is credited with coining the term 'Big Bang' in 1949 during a BBC broadcast, though he supported the competing steady-state model. [cite: 126, 152]\",\n",
    "     \"Observations of distant supernovae revealed that the universeâ€™s expansion is accelerating, leading to the concept of dark energy. [cite: 36, 37]\"\n",
    "]\n",
    "simulated_metadatas = [\n",
    "    {'source': 'Big Bang.txt', 'chunk_id': 1},\n",
    "    {'source': 'Concept History-Big Bang.txt', 'chunk_id': 5},\n",
    "    {'source': 'Big Bang.txt', 'chunk_id': 4}\n",
    "]\n",
    "\n",
    "embedder = Embedder()\n",
    "vector_db = VectorDB(simulated_texts, simulated_metadatas)\n",
    "llm = LLM()\n",
    "\n",
    "# --- Completed RAG Query Function ---\n",
    "\n",
    "def rag_query(question: str, top_k: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Answer a question using RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "    Args:\n",
    "        question: The user's question\n",
    "        top_k: How many chunks to retrieve\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - 'question': the original question\n",
    "        - 'answer': the LLM's answer\n",
    "        - 'sources': list of source filenames\n",
    "        - 'contexts': list of retrieved chunks\n",
    "        - 'time': how long it took\n",
    "    \"\"\"\n",
    "\n",
    "    # Start timer\n",
    "    timer = Timer()\n",
    "    timer.start()\n",
    "\n",
    "    # Step 1: Embed question\n",
    "    query_embedding = embedder.embed_text(question)\n",
    "\n",
    "    # Step 2: Search database\n",
    "    # In a real environment, query_embedding is the input\n",
    "    results = vector_db.search(query_embedding, top_k)\n",
    "\n",
    "    # Step 3: Extract results (Note: ChromaDB returns nested lists)\n",
    "    # The documents and metadatas lists are usually the first element in the result list.\n",
    "    retrieved_chunks = results['documents'][0]\n",
    "    retrieved_metadata = results['metadatas'][0]\n",
    "\n",
    "    # Step 4: Build context\n",
    "    context = '\\n\\n'.join(retrieved_chunks)\n",
    "\n",
    "    # Step 5: Create prompt (use this template)\n",
    "    prompt = f\"\"\"Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Step 6: Generate answer\n",
    "    answer = llm.generate_answer(prompt)\n",
    "\n",
    "    # Step 7: Extract sources (only unique filenames)\n",
    "    # Use a set comprehension for efficient and unique source filenames\n",
    "    sources = list(set([meta['source'] for meta in retrieved_metadata]))\n",
    "\n",
    "    # Stop timer\n",
    "    elapsed_time = timer.stop()\n",
    "\n",
    "    # Step 8: Return results\n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'sources': sources,\n",
    "        'contexts': retrieved_chunks,\n",
    "        'time': elapsed_time\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"âœ“ RAG query function defined!\")\n",
    "\n",
    "# --- Testing the Function ---\n",
    "test_question = \"Who first proposed the idea of a 'primeval atom'?\"\n",
    "rag_results = rag_query(test_question, top_k=2)\n",
    "\n",
    "print(\"\\n--- RAG Results Summary ---\")\n",
    "print(f\"Question: {rag_results['question']}\")\n",
    "print(f\"Answer: {rag_results['answer']}\")\n",
    "print(f\"Sources Used: {rag_results['sources']}\")\n",
    "print(f\"Time taken: {rag_results['time']} seconds\")\n",
    "print(f\"\\nContext 1 (for verification): {rag_results['contexts'][0][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your RAG System!\n",
    "\n",
    "Let's try asking some questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 1\n",
    "result = rag_query(\"What are the attendance rules?\")\n",
    "\n",
    "# Pretty print the result\n",
    "print_rag_answer(\n",
    "    result['question'],\n",
    "    result['answer'],\n",
    "    result['sources'],\n",
    "    result['time']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own question!\n",
    "my_question = \"What happens if you cheat?\"  # Change this!\n",
    "\n",
    "result = rag_query(my_question)\n",
    "print_rag_answer(\n",
    "    result['question'],\n",
    "    result['answer'],\n",
    "    result['sources'],\n",
    "    result['time']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #5: Create Test Dataset\n",
    "\n",
    "**Your Task:** Create a list of test questions to evaluate your RAG system.\n",
    "\n",
    "**What to do:**\n",
    "1. Think of 10 questions your documents can answer\n",
    "2. For each question, write the expected answer\n",
    "3. Store them in a structured format\n",
    "\n",
    "**Python concepts:** Lists, dictionaries, data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your test dataset!\n",
    "# HINTS:\n",
    "# Create a list of dictionaries\n",
    "# Each dictionary should have:\n",
    "#   - 'question': the test question\n",
    "#   - 'expected_answer': what you expect the answer to include\n",
    "#   - 'category': type of question (factual, inferential, etc.)\n",
    "\n",
    "test_questions = [\n",
    "    # Example (replace with your own!):\n",
    "    {\n",
    "        'question': 'What are the attendance rules?',\n",
    "        'expected_answer': 'Students must attend all classes, excused absences need parent notification',\n",
    "        'category': 'factual'\n",
    "    },\n",
    "    # Add 9 more questions here!\n",
    "    # Your code here:\n",
    "\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Created {len(test_questions)} test questions\")\n",
    "print(f\"\\nExample question:\")\n",
    "print(f\"  Q: {test_questions[0]['question']}\")\n",
    "print(f\"  Expected: {test_questions[0]['expected_answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #6: Calculate Evaluation Metrics\n",
    "\n",
    "**Your Task:** Write functions to measure how well your RAG system performs.\n",
    "\n",
    "**Python concepts:** Functions, calculations, statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_latency(results: List[Dict]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate average response time.\n",
    "\n",
    "    Args:\n",
    "        results: List of result dictionaries (each has 'time' field)\n",
    "\n",
    "    Returns:\n",
    "        Average time in seconds\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "    # HINTS:\n",
    "    # 1. Extract all 'time' values from results\n",
    "    # 2. Sum them up\n",
    "    # 3. Divide by the number of results\n",
    "    # 4. Return the average\n",
    "\n",
    "    # Your code here:\n",
    "    total_time = 0\n",
    "    # Calculate sum and average\n",
    "\n",
    "    return 0.0  # Replace with your calculation\n",
    "\n",
    "\n",
    "def count_successful_retrievals(results: List[Dict]) -> int:\n",
    "    \"\"\"\n",
    "    Count how many queries successfully retrieved context.\n",
    "\n",
    "    Args:\n",
    "        results: List of result dictionaries\n",
    "\n",
    "    Returns:\n",
    "        Number of successful retrievals\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "    # HINTS:\n",
    "    # 1. Start with count = 0\n",
    "    # 2. For each result:\n",
    "    #    - Check if 'contexts' is not empty\n",
    "    #    - If yes, increment count\n",
    "    # 3. Return count\n",
    "\n",
    "    # Your code here:\n",
    "    count = 0\n",
    "    # Count successful retrievals\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def get_all_sources(results: List[Dict]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get unique list of all sources used.\n",
    "\n",
    "    Args:\n",
    "        results: List of result dictionaries\n",
    "\n",
    "    Returns:\n",
    "        List of unique source filenames\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "    # HINTS:\n",
    "    # 1. Create an empty set (sets automatically keep unique values)\n",
    "    # 2. For each result:\n",
    "    #    - Get the 'sources' list\n",
    "    #    - Add each source to the set\n",
    "    # 3. Convert set to list and return\n",
    "\n",
    "    # Your code here:\n",
    "    all_sources = set()\n",
    "    # Collect all sources\n",
    "\n",
    "    return list(all_sources)\n",
    "\n",
    "\n",
    "print(\"âœ“ Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #7: Run Complete Evaluation\n",
    "\n",
    "**Your Task:** Test your RAG system with all test questions and calculate metrics.\n",
    "\n",
    "**Python concepts:** Loops, function calls, data aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(test_questions: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run RAG system on all test questions.\n",
    "\n",
    "    Args:\n",
    "        test_questions: List of test question dictionaries\n",
    "\n",
    "    Returns:\n",
    "        List of result dictionaries\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "    # HINTS:\n",
    "    # 1. Create empty list for results\n",
    "    # 2. For each test question:\n",
    "    #    - Get the question text\n",
    "    #    - Call rag_query() with the question\n",
    "    #    - Add result to results list\n",
    "    # 3. Return results\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Your code here:\n",
    "    for test in test_questions:\n",
    "        # Get question\n",
    "        # Run RAG query\n",
    "        # Store result\n",
    "        pass  # Remove this 'pass' and write your code\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run evaluation on all test questions\n",
    "print(\"Running evaluation on all test questions...\\n\")\n",
    "all_results = run_evaluation(test_questions)\n",
    "\n",
    "print(f\"\\nâœ“ Completed {len(all_results)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Display Results\n",
    "\n",
    "Show the evaluation metrics and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics using your functions\n",
    "avg_latency = calculate_average_latency(all_results)\n",
    "successful_retrievals = count_successful_retrievals(all_results)\n",
    "all_sources_used = get_all_sources(all_results)\n",
    "hit_rate = successful_retrievals / len(all_results) if all_results else 0\n",
    "\n",
    "# Display metrics\n",
    "print_separator(\"EVALUATION RESULTS\")\n",
    "print(f\"\\nTotal Questions Tested: {len(all_results)}\")\n",
    "print(f\"Successful Retrievals: {successful_retrievals}\")\n",
    "print(f\"Hit Rate: {hit_rate:.2%}\")\n",
    "print(f\"Average Latency: {avg_latency:.2f} seconds\")\n",
    "print(f\"\\nSources Used: {', '.join(all_sources_used)}\")\n",
    "print_separator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display individual results\n",
    "print(\"\\nIndividual Test Results:\\n\")\n",
    "\n",
    "for i, result in enumerate(all_results, 1):\n",
    "    print(f\"[Test {i}]\")\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Answer: {result['answer'][:200]}...\")\n",
    "    print(f\"Sources: {', '.join(set(result['sources']))}\")\n",
    "    print(f\"Time: {result['time']:.2f}s\")\n",
    "    print(\"-\" * 60)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Your Results\n",
    "\n",
    "Save your test results to a JSON file for your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON file\n",
    "results_summary = {\n",
    "    'metrics': {\n",
    "        'total_questions': len(all_results),\n",
    "        'successful_retrievals': successful_retrievals,\n",
    "        'hit_rate': hit_rate,\n",
    "        'average_latency': avg_latency\n",
    "    },\n",
    "    'results': all_results\n",
    "}\n",
    "\n",
    "with open('evaluation_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Results saved to 'evaluation_results.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations! ðŸŽ‰\n",
    "\n",
    "You've successfully built a RAG system!\n",
    "\n",
    "### What You Accomplished:\n",
    "âœ… Loaded documents from files  \n",
    "âœ… Chunked text with overlap  \n",
    "âœ… Created a RAG query pipeline  \n",
    "âœ… Built a test dataset  \n",
    "âœ… Calculated evaluation metrics  \n",
    "âœ… Generated a results report  \n",
    "\n",
    "### Next Steps:\n",
    "- Try different chunk sizes and overlaps\n",
    "- Add more test questions\n",
    "- Experiment with different values for `top_k`\n",
    "- Analyze which questions work best\n",
    "- Write up your findings in a report\n",
    "\n",
    "### For Your Report:\n",
    "1. Describe your document collection\n",
    "2. Explain your chunking strategy\n",
    "3. Present your evaluation metrics\n",
    "4. Show examples of good and bad answers\n",
    "5. Discuss what you learned\n",
    "\n",
    "Great job! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
